{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing introductory challenge\n",
    "This challenge was built to introduce someone to applying machine learning to problems of natural language processing. In particular, it aims at detecting natural disasters from tweets. I will use keras' arsenal to clean the dataset from punctuation, @, and other symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "from sklearn import feature_extraction, linear_model, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Datasets/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"../Datasets/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analzying the dataset\n",
    "First I want to see if there are obvious words that are not connected to natural catastrophes, using the `keyword` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">id</th>\n",
       "      <th colspan=\"8\" halign=\"left\">target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ablaze</th>\n",
       "      <td>36.0</td>\n",
       "      <td>70.388889</td>\n",
       "      <td>14.035216</td>\n",
       "      <td>48.0</td>\n",
       "      <td>58.50</td>\n",
       "      <td>69.5</td>\n",
       "      <td>81.25</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.487136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accident</th>\n",
       "      <td>35.0</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>15.118746</td>\n",
       "      <td>96.0</td>\n",
       "      <td>109.50</td>\n",
       "      <td>121.0</td>\n",
       "      <td>134.50</td>\n",
       "      <td>145.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.471008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aftershock</th>\n",
       "      <td>34.0</td>\n",
       "      <td>171.323529</td>\n",
       "      <td>13.975564</td>\n",
       "      <td>146.0</td>\n",
       "      <td>160.25</td>\n",
       "      <td>171.5</td>\n",
       "      <td>182.75</td>\n",
       "      <td>195.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airplane%20accident</th>\n",
       "      <td>35.0</td>\n",
       "      <td>220.142857</td>\n",
       "      <td>15.406536</td>\n",
       "      <td>196.0</td>\n",
       "      <td>208.50</td>\n",
       "      <td>219.0</td>\n",
       "      <td>233.50</td>\n",
       "      <td>245.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.355036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ambulance</th>\n",
       "      <td>38.0</td>\n",
       "      <td>269.052632</td>\n",
       "      <td>14.101845</td>\n",
       "      <td>246.0</td>\n",
       "      <td>258.50</td>\n",
       "      <td>268.5</td>\n",
       "      <td>279.75</td>\n",
       "      <td>294.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.506009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wounded</th>\n",
       "      <td>37.0</td>\n",
       "      <td>10609.135135</td>\n",
       "      <td>14.491688</td>\n",
       "      <td>10585.0</td>\n",
       "      <td>10598.00</td>\n",
       "      <td>10609.0</td>\n",
       "      <td>10622.00</td>\n",
       "      <td>10632.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.463373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wounds</th>\n",
       "      <td>33.0</td>\n",
       "      <td>10662.393939</td>\n",
       "      <td>14.225724</td>\n",
       "      <td>10636.0</td>\n",
       "      <td>10651.00</td>\n",
       "      <td>10663.0</td>\n",
       "      <td>10675.00</td>\n",
       "      <td>10684.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.466694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wreck</th>\n",
       "      <td>37.0</td>\n",
       "      <td>10708.513514</td>\n",
       "      <td>15.230856</td>\n",
       "      <td>10685.0</td>\n",
       "      <td>10695.00</td>\n",
       "      <td>10708.0</td>\n",
       "      <td>10722.00</td>\n",
       "      <td>10733.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.397061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wreckage</th>\n",
       "      <td>39.0</td>\n",
       "      <td>10759.717949</td>\n",
       "      <td>14.730828</td>\n",
       "      <td>10735.0</td>\n",
       "      <td>10747.50</td>\n",
       "      <td>10760.0</td>\n",
       "      <td>10771.50</td>\n",
       "      <td>10784.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrecked</th>\n",
       "      <td>39.0</td>\n",
       "      <td>10810.692308</td>\n",
       "      <td>15.178159</td>\n",
       "      <td>10785.0</td>\n",
       "      <td>10798.50</td>\n",
       "      <td>10812.0</td>\n",
       "      <td>10823.50</td>\n",
       "      <td>10834.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.269953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                              \\\n",
       "                    count          mean        std      min       25%   \n",
       "keyword                                                                 \n",
       "ablaze               36.0     70.388889  14.035216     48.0     58.50   \n",
       "accident             35.0    121.800000  15.118746     96.0    109.50   \n",
       "aftershock           34.0    171.323529  13.975564    146.0    160.25   \n",
       "airplane%20accident  35.0    220.142857  15.406536    196.0    208.50   \n",
       "ambulance            38.0    269.052632  14.101845    246.0    258.50   \n",
       "...                   ...           ...        ...      ...       ...   \n",
       "wounded              37.0  10609.135135  14.491688  10585.0  10598.00   \n",
       "wounds               33.0  10662.393939  14.225724  10636.0  10651.00   \n",
       "wreck                37.0  10708.513514  15.230856  10685.0  10695.00   \n",
       "wreckage             39.0  10759.717949  14.730828  10735.0  10747.50   \n",
       "wrecked              39.0  10810.692308  15.178159  10785.0  10798.50   \n",
       "\n",
       "                                                target                      \\\n",
       "                         50%       75%      max  count      mean       std   \n",
       "keyword                                                                      \n",
       "ablaze                  69.5     81.25     95.0   36.0  0.361111  0.487136   \n",
       "accident               121.0    134.50    145.0   35.0  0.685714  0.471008   \n",
       "aftershock             171.5    182.75    195.0   34.0  0.000000  0.000000   \n",
       "airplane%20accident    219.0    233.50    245.0   35.0  0.857143  0.355036   \n",
       "ambulance              268.5    279.75    294.0   38.0  0.526316  0.506009   \n",
       "...                      ...       ...      ...    ...       ...       ...   \n",
       "wounded              10609.0  10622.00  10632.0   37.0  0.702703  0.463373   \n",
       "wounds               10663.0  10675.00  10684.0   33.0  0.303030  0.466694   \n",
       "wreck                10708.0  10722.00  10733.0   37.0  0.189189  0.397061   \n",
       "wreckage             10760.0  10771.50  10784.0   39.0  1.000000  0.000000   \n",
       "wrecked              10812.0  10823.50  10834.0   39.0  0.076923  0.269953   \n",
       "\n",
       "                                              \n",
       "                     min  25%  50%  75%  max  \n",
       "keyword                                       \n",
       "ablaze               0.0  0.0  0.0  1.0  1.0  \n",
       "accident             0.0  0.0  1.0  1.0  1.0  \n",
       "aftershock           0.0  0.0  0.0  0.0  0.0  \n",
       "airplane%20accident  0.0  1.0  1.0  1.0  1.0  \n",
       "ambulance            0.0  0.0  1.0  1.0  1.0  \n",
       "...                  ...  ...  ...  ...  ...  \n",
       "wounded              0.0  0.0  1.0  1.0  1.0  \n",
       "wounds               0.0  0.0  0.0  1.0  1.0  \n",
       "wreck                0.0  0.0  0.0  0.0  1.0  \n",
       "wreckage             1.0  1.0  1.0  1.0  1.0  \n",
       "wrecked              0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[221 rows x 16 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('keyword').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that there are some words, such as `aftershock`, which are never related to a natural disaster. At the same time, the word `wreckage` is always attached to it. Both have more than 30 occurrences. Is there a way to code this into my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "## Standardization\n",
    "Here we standardize the data to remove punctuation and other confusing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = input_data.lower()\n",
    "    stripped_handle = re.sub('@\\w+', '', lowercase)\n",
    "    remove_hashtag = re.sub('#', '', stripped_handle)\n",
    "    remove_punctuation = re.sub('[%s]' % re.escape(string.punctuation), '', remove_hashtag)\n",
    "    return re.sub('(https|http)\\w+', '', remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, first takes out all capitalization, then the regex snippets replace all handles (any word that starts with `@`), eliminate the `#` by keeping the hashtag handle, removes punctuation and finally removes possible web addresses. _E.g._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion https://t.co/gFJfgTodad\n",
      " bahrain police had previously died in a road accident they were not killed by explosion \n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"text\"][100])\n",
    "\n",
    "print(custom_standardization(train_df[\"text\"][100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"clean text\"] = train_df[\"text\"].map(custom_standardization)\n",
    "test_df[\"clean text\"] = test_df[\"text\"].map(custom_standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Time to apply this to generate the dataset like in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"clean text\"])\n",
    "# to have the same vectors in the test_vectors, we use transform instead of fit_transform\n",
    "test_vectors = count_vectorizer.transform(test_df[\"clean text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15588)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors[0].todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduced the number of generated tokens by about 6000, which isn't half bad (reduction of almost 30%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: ridge regression\n",
    "We can already check whether the ridge regression works better on cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61209964, 0.49491525, 0.55414013, 0.55190311, 0.65375678])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.RidgeClassifier()\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not seem particularly better! That's quite interesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeClassifier()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../Datasets/nlp-getting-started/sample_submission.csv\")\n",
    "\n",
    "sample_submission[\"target\"] = clf.predict(test_vectors)\n",
    "sample_submission.to_csv(\"ridge_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is worse than without cleaning the data! That's surprising. I wonder if that might be due to some information linked to hashtags. Maybe we should add an indicator of whether a word contained hashtags or not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('machinelearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25ad6a4be8b7cb5307e079b5c70053dc031a1417502bda203100eef0d9318af0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
