{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study case: the 2nd classified solution of the challenge\n",
    "\n",
    "Ideally in this notebook I will do 3 things:\n",
    "1) study one of the top solutions of the challenge\n",
    "2) replicate the feature engineering \n",
    "3) implement a version of the solution, as well as other models such as a DNN\n",
    "\n",
    "## 2nd classified solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few global variables are set. One is the path to the training and test data, the other is a state to remove randomness of some algorithms (for reproduction purposes). In the same cell, we define the features and the target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Datasets/killer-shrimp-invasion/'\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "train = pd.read_csv(data_dir + 'train.csv')\n",
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "\n",
    "# split data\n",
    "Y_train = train['Presence']\n",
    "ID_train = train['pointid']\n",
    "X_train = train.drop(['Presence', 'pointid'], axis=1)\n",
    "ID_test = test['pointid']\n",
    "X_test = test.drop(['pointid'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "As I previously attested in my own notebook, there is quite a lot of missing data. Using `sklearn`'s `IterativeImputer` we can impute values for missing data. It performs better than filling in with the mean. However, I'm lazy and I'm perfectly fine filling it with the mean..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "There are 5 features in the training data: temperature, salinity, depth, exposure and subtrate. WE create more features:\n",
    "- Ocean Density: function of temperature, salinity and depth based on UNESCO formula\n",
    "- EUNIS Exposure Classifications (nine features): this is a categorical based on exposure\n",
    "- Temperature Equation: this is basically an equation obtained from fitting the training data using only the temperature as a feature. We use the response of this fitted equation as additional feature.\n",
    "- Outlier (Bounded-Box): this are the conditions of the normal habitat of Killer Shrimps hard coded into the problem.\n",
    "    - tolerate salinity up to 20ppt (12ppt is optimal)\n",
    "    - tolerate temperatures between 0 and 35C (5-15 optimal)\n",
    "    - thought to occupy every substratum except sand\n",
    "    - present in areas of low current velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ocean density:\n",
    "def ocean_density(temp, salin, depth):\n",
    "    pressure_approx = 1 + (depth / 10)\n",
    "\n",
    "    # standard mean ocean water density (smow)\n",
    "    smow = 999.842594 + (6.793953e-2 * temp) - (9.095290e-3 * (temp**2)) + \\\n",
    "           (1.001685e-4 * (temp**3)) - (1.120083e-6 * (temp**4)) + (6.536332e-9 * (temp**5))\n",
    "\n",
    "    # sea water density at normal atmospheric pressure\n",
    "    B = 0.82449 - (4.0899e-3 * temp) + (7.6438e-5 * (temp**2)) + (8.2467e-7 * (temp**3)) + \\\n",
    "        (5.3875e-9 * (temp**4))\n",
    "    C = -5.7246e-3 + (1.0227e-4 * temp) + (-1.6546e-6 * (temp**2))\n",
    "    D = 4.8314e-4\n",
    "    density_normal = smow + (B * salin) + (C * (salin**1.5)) + (D * (salin**2))\n",
    "\n",
    "    # determination of compression module at pression 0\n",
    "    Kw = 19652.21 + (148.4206 * temp) + (-2.327105 * (temp**2)) + (1.360477e-2 * (temp**3)) + \\\n",
    "        (-5.155288e-5 * (temp**4))\n",
    "    F = 52.6746 + (-0.603459 * temp) + (1.099870e-2 * (temp**2)) + (-6.167e-5 * (temp**3))\n",
    "    G = 7.9440e-2 + (1.6483e-2 * temp) + (-5.3009e-4 * (temp**2))\n",
    "    K_0 = Kw + (F * salin) + (G * (salin**1.5))\n",
    "\n",
    "    # determination of final compressibility module\n",
    "    Aw = 3.23990 + (1.43713e-3 * temp) + (1.16092e-4 * (temp**2)) + (-5.77905e-7 * (temp**3))\n",
    "    A1 = Aw + ((2.28380e-3 + (-1.09810e-5 * temp) + (1-60780e-6 * (temp**2))) * salin) + (1.91075e-4 * (salin ** 1.5))\n",
    "    Bw = 8.50935e-5 + (-6.12293e-6 * temp) + (5.27870e-8 * (temp**2))\n",
    "    B2 = Bw + (-9.9348e-7 + (2.0816e-8 * temp) + (9.1697e-10 * (temp**2))) * salin \n",
    "    K = K_0 + (A1*pressure_approx) + (B2*(pressure_approx**2))\n",
    "\n",
    "    return (density_normal / (1- (pressure_approx / K))) - 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Density'] = X_train.apply(lambda x: ocean_density(x['Temperature_today'], x['Salinity_today'], x['Depth']), axis=1)\n",
    "X_test['Density'] = X_test.apply(lambda x: ocean_density(x['Temperature_today'], x['Salinity_today'], x['Depth']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EUNIS Exposure Classifications\n",
    "\n",
    "def eunis_classification(exposure):\n",
    "    if exposure <= 1200:\n",
    "        return 'Ultra Sheltered'\n",
    "    elif exposure <= 4000:\n",
    "        return 'Extremely Sheltered'\n",
    "    elif exposure <= 10000:\n",
    "        return 'Very Sheltered'\n",
    "    elif exposure <= 100000:\n",
    "        return 'Sheltered'\n",
    "    elif exposure <= 500000:\n",
    "        return 'Moderately Exposed'\n",
    "    elif exposure <= 1000000:\n",
    "        return 'Exposed'\n",
    "    elif exposure <= 2000000:\n",
    "        return 'Very Exposed'\n",
    "    else:\n",
    "        return 'Extremely Exposed'\n",
    "\n",
    "for classifi in ['Ultra Sheltered', 'Extremely Sheltered', 'Very Sheltered', 'Sheltered', \n",
    "                 'Moderately Exposed', 'Exposed', 'Very Exposed']:\n",
    "    X_train['EUNIS ' + classifi] = X_train['Exposure'].apply(lambda x: 1 if eunis_classification(x) == classifi else 0)\n",
    "    X_test['EUNIS ' + classifi] = X_test['Exposure'].apply(lambda x: 1 if eunis_classification(x) == classifi else 0)\n",
    "\n",
    "X_train['EUNIS Min Very Exposed'] = X_train['Exposure'].apply(lambda x: 1 if eunis_classification(x) == 'Extremely Exposed' \n",
    "                                                              or eunis_classification(x) == 'Very Exposed' else 0)\n",
    "X_test['EUNIS Min Very Exposed'] = X_test['Exposure'].apply(lambda x: 1 if eunis_classification(x) == 'Extremely Exposed' \n",
    "                                                              or eunis_classification(x) == 'Very Exposed' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature model\n",
    "def temperature_equation(t):\n",
    "    coefs = [-10.52910058, 2.21529641, -0.57623745, -2.06832485, 0.30713969, -2.21781761, -1.85363275]\n",
    "    temps = [1, t, t**2, t**3, t**4, t**5, t**6]\n",
    "    summed = np.dot(coefs, temps)\n",
    "    return 1 / (1 + np.exp(summed))\n",
    "\n",
    "X_train['Temperature Model'] = X_train['Temperature_today'].apply(lambda x: temperature_equation(x))\n",
    "X_test['Temperature Model'] = X_test['Temperature_today'].apply(lambda x: temperature_equation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers (bounded box)\n",
    "class OutlierFeature():\n",
    "    def __init__(self, ratio, outlier_features):\n",
    "        self.ratio = ratio\n",
    "        self.outlier_features = outlier_features\n",
    "    \n",
    "    def exclude_outliers(self, row):\n",
    "        lower_multiplier = self.ratio\n",
    "        upper_multiplier = 1 / self.ratio\n",
    "        conditions = []\n",
    "        for feature, cmin, cmax in self.outlier_conditions:\n",
    "            conditions.append(row[feature] > cmin)\n",
    "            conditions.append(row[feature] < cmax)\n",
    "        return 1 if sum(conditions) == len(conditions) else 0\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.outlier_conditions = []\n",
    "        for feature in self.outlier_features:\n",
    "            fmin = x[y==1][feature].min()\n",
    "            fmax = x[y==1][feature].max()\n",
    "            lower_multiplier = self.ratio\n",
    "            upper_multiplier = 1 / self.ratio\n",
    "\n",
    "            cmin = fmin * lower_multiplier if fmin > 0 else fmin * upper_multiplier\n",
    "            cmax = fmax * upper_multiplier if fmax > 0 else fmax * lower_multiplier\n",
    "            self.outlier_conditions.append((feature, cmin, cmax))\n",
    "\n",
    "    def transform(self, x):\n",
    "        outliers = x.apply(lambda x: self.exclude_outliers(x), axis = 1)\n",
    "        return outliers\n",
    "\n",
    "outlier_finder = OutlierFeature(0.82, ['Temperature_today', 'Salinity_today', 'Exposure', 'Depth'])\n",
    "outlier_finder.fit(X_train, Y_train)\n",
    "\n",
    "X_train['Outlier'] = outlier_finder.transform(X_train)\n",
    "X_test['Outlier'] = outlier_finder.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In the next few steps, we define a function allowing to perform statified cross validation. This consists in evaluating cross-validation with stratified sampling (that is, sampling performed making sure that the percentage of a certain value is the same as in the dataset). For this, 5-fold cross-validation is used. It also prints out the AUROC score. The ROC curve is the plot obtained by plotting the True Positive Rate (sensitivity) to the false positive rate (1 - specificity), and AUROC is the Area Under the Receiver Operating Characteristic. If the area is 1, the model is perfect. If AUROC = 0.5, it operates like a random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_fold_cv(model, X_train, Y_train, verbose = True):\n",
    "    skf = StratifiedKFold(n_splits = 5)\n",
    "    fold = 1\n",
    "    scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X_train, Y_train):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train_fold, Y_train_fold)\n",
    "        \n",
    "        preds = model.predict_proba(X_test_fold)\n",
    "        preds = [x[1] for x in preds]\n",
    "\n",
    "        score = roc_auc_score(Y_test_fold, preds)\n",
    "        scores.append(score)\n",
    "        if verbose:\n",
    "            print('Fold', fold, '    ', score)\n",
    "        fold += 1\n",
    "\n",
    "    avg = np.mean(scores)\n",
    "    if verbose:\n",
    "        print()\n",
    "        print('Average', avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define the list of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Temperature_today', 'Salinity_today', 'Depth', 'EUNIS Ultra Sheltered', 'EUNIS Extremely Sheltered', \n",
    "            'EUNIS Very Sheltered', 'EUNIS Sheltered', 'EUNIS Moderately Exposed', 'EUNIS Exposed', 'EUNIS Very Exposed', \n",
    "            'Substrate', 'Outlier', 'Temperature Model', 'Density', 'EUNIS Min Very Exposed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to generate an XGB model (eXtreme Gradient Boosting). Gradient boosting is a ML technique used in regression and classification tasks. It gives a prediction model in the form of an ensemble of weak prediction models (typically, decision trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(random_state = RANDOM_STATE, eval_metric = 'auc', objective = 'binary:logistic',\n",
    "                      learning_rate = 0.3, max_depth = 5, subsample = 1, reg_lambda = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1      0.9996289025683597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2      0.9996640321097338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3      0.9987628498911841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4      0.9994979046037786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5      0.9994874314068218\n",
      "\n",
      "Average 0.9994082241159756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = five_fold_cv(model, X_train[features], Y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\xtomma\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    }
   ],
   "source": [
    "preds = pd.DataFrame(ID_test, columns=['pointid'])\n",
    "\n",
    "model.fit(X_train[features], Y_train)\n",
    "preds['Presence'] = model.predict_proba(X_test[features])[:,1]\n",
    "\n",
    "#preds[['pointid', 'Presence']].to_csv('preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a very interesting result! Now in another notebook I will look at the same approach without feature engineering and post it to gauge how much it can be gained really from it. Feature engineering was very cumbersome, and it might be worth it for minimal gains, but not really for giving a scientifically useful answer..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('machinelearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25ad6a4be8b7cb5307e079b5c70053dc031a1417502bda203100eef0d9318af0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
